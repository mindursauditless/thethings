// generate-module-page.js ‚Äî patched to send large content in chunks for Assistants API

const fetch = require('node-fetch');
const fs = require('fs');
const path = require('path');
const { OpenAI } = require('openai');
const loadModulePrompt = require('./moduleprompt');
const { uploadMarkdownToSupabase } = require('./upload-markdown-to-supabase');
require('dotenv').config();

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const SUPABASE_PROJECT = process.env.SUPABASE_URL.replace('https://', '');
const RAW_BUCKET = 'raw-inputs';
const MAX_CHUNK_SIZE = 950_000; // leave buffer for headers

async function generateModulePage(thread_id, moduleName) {
  try {
    console.log(`üß† Auditing module: ${moduleName}`);

    const fileUrl = `https://${SUPABASE_PROJECT}/storage/v1/object/public/${RAW_BUCKET}/raw/${thread_id}/${moduleName}.json`;
    const fileRes = await fetch(fileUrl);
    if (!fileRes.ok) {
      console.log(`‚è© Skipping ${moduleName} ‚Äî could not fetch file (${fileRes.status})`);
      return;
    }

    const rows = await fileRes.json();
    console.log(`üì¶ ${rows.length} rows loaded for module '${moduleName}'`);

    if (!rows.length) {
      console.log(`‚è© Skipping ${moduleName} ‚Äî no rows to analyze.`);
      return;
    }

    const prompt = loadModulePrompt(moduleName, rows);

    // Split long text into smaller chunks
    const thread = await openai.beta.threads.create();
    console.log(`üßµ Created thread: ${thread.id}`);

    const chunks = [];
    for (let i = 0; i < prompt.length; i += MAX_CHUNK_SIZE) {
      chunks.push(prompt.slice(i, i + MAX_CHUNK_SIZE));
    }

    for (const chunk of chunks) {
      await openai.beta.threads.messages.create(thread.id, {
        role: 'user',
        content: [{ type: 'text', text: chunk }]
      });
    }

    const run = await openai.beta.threads.runs.create(thread.id, {
      assistant_id: process.env.OPENAI_ASSISTANT_ID,
      instructions: 'Only return valid Markdown, formatted as a strategy report.'
    });

    let runStatus = run.status;
    while (runStatus !== 'completed' && runStatus !== 'failed') {
      await new Promise(resolve => setTimeout(resolve, 2000));
      const statusCheck = await openai.beta.threads.runs.retrieve(thread.id, run.id);
      runStatus = statusCheck.status;
    }

    if (runStatus !== 'completed') {
      throw new Error('GPT run failed');
    }

    const messages = await openai.beta.threads.messages.list(thread.id);
    const assistantMsg = messages.data.find(m => m.role === 'assistant');
    const gptContent = assistantMsg?.content?.[0]?.text?.value?.trim();

    if (!gptContent || gptContent.length < 10) {
      throw new Error('GPT returned no usable content');
    }

    const reportsDir = path.join(__dirname, 'reports');
    if (!fs.existsSync(reportsDir)) fs.mkdirSync(reportsDir);

    const filePath = path.join(reportsDir, `${thread_id}--${moduleName}.md`);
    fs.writeFileSync(filePath, gptContent, 'utf8');
    console.log(`‚úÖ Saved report: /reports/${thread_id}--${moduleName}.md`);

    console.log(`üì§ Attempting upload for ${thread_id}--${moduleName}.md`);
    const url = await uploadMarkdownToSupabase(thread_id, moduleName);

    if (url) {
      console.log(`üîó Supabase URL: ${url}`);
    } else {
      console.log(`‚ùå Upload failed or returned null`);
    }

    return url;
  } catch (err) {
    console.error(`üî• Error generating module report for ${moduleName}:`, err);
  }
}

async function generateAllModules(thread_id, modules = []) {
  const links = [];
  for (const module of modules) {
    const url = await generateModulePage(thread_id, module);
    if (url) links.push({ module, url });
  }
  console.log(`‚è±Ô∏è Total classification time: ${new Date().toLocaleTimeString()}`);
  return links;
}

module.exports = { generateModulePage, generateAllModules };
